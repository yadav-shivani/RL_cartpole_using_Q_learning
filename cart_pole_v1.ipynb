{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpmAIJfvmScJ36PiVhAcJM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yadav-shivani/RL_cartpole_using_Q_learning/blob/main/cart_pole_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JoNkrHmJ5RkE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env=gym.make('CartPole-v1')\n",
        "print(env.spec)\n",
        "#print(env.action_space.n)\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.95\n",
        "EPISODES = 60000\n",
        "total = 0\n",
        "total_reward = 0\n",
        "prior_reward = 0\n",
        "Observation = [30, 30, 50, 50]\n",
        "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
        "epsilon = 1\n",
        "epsilon_decay_value = 0.99995"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_ZAfeP-GEOx",
        "outputId": "f7f0ec44-8f94-4476-b054-ddfd5263648d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EnvSpec(id='CartPole-v1', entry_point='gym.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, autoreset=False, disable_env_checker=False, new_step_api=False, kwargs={}, namespace=None, name='CartPole', version=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
        "q_table.shape\n",
        "def get_discrete_state(state):\n",
        "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
        "    return tuple(discrete_state.astype(np.int))"
      ],
      "metadata": {
        "id": "a-mH1DJnIeIr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(EPISODES + 1): #go through the episodes\n",
        "    t0 = time.time() #set the initial time\n",
        "    discrete_state = get_discrete_state(env.reset()) #get the discrete start for the restarted environment\n",
        "    done = False\n",
        "    episode_reward = 0 #reward starts as 0 for each episode\n",
        "    if episode % 2000 == 0:\n",
        "        print(\"Episode: \" + str(episode))\n",
        "    while not done:\n",
        "\n",
        "        if np.random.random() > epsilon:\n",
        "\n",
        "            action = np.argmax(q_table[discrete_state]) #take co-ordinated action\n",
        "        else:\n",
        "\n",
        "            action = np.random.randint(0, env.action_space.n) #do a random action\n",
        "\n",
        "        new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
        "\n",
        "        episode_reward += reward #add the reward\n",
        "\n",
        "        new_discrete_state = get_discrete_state(new_state)\n",
        "\n",
        "        # if episode % 2000 == 0: # render\n",
        "        #     env.render()\n",
        "\n",
        "        if not done: # update q-table\n",
        "            max_future_q = np.max(q_table[new_discrete_state])\n",
        "\n",
        "            current_q = q_table[discrete_state + (action,)]\n",
        "\n",
        "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
        "\n",
        "            q_table[discrete_state + (action,)] = new_q\n",
        "\n",
        "        discrete_state = new_discrete_state\n",
        "\n",
        "    if epsilon > 0.05: # epsilon modification\n",
        "        if episode_reward > prior_reward and episode > 10000:\n",
        "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
        "\n",
        "            if episode % 500 == 0:\n",
        "                print(\"Epsilon: \" + str(epsilon))\n",
        "\n",
        "    t1 = time.time() # episode has finished\n",
        "    episode_total = t1 - t0 # episode total time\n",
        "    total = total + episode_total\n",
        "\n",
        "    total_reward += episode_reward # episode total reward\n",
        "    prior_reward = episode_reward\n",
        "\n",
        "    if episode % 1000 == 0: # every 1000 episodes print the average time and the average reward\n",
        "        mean = total / 1000\n",
        "        print(\"Time Average: \" + str(mean))\n",
        "        total = 0\n",
        "\n",
        "        mean_reward = total_reward / 1000\n",
        "        print(\"Mean Reward: \" + str(mean_reward))\n",
        "        total_reward = 0\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFToFyPH1FHj",
        "outputId": "e537c6ac-c712-46a4-c186-482df33dd078"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-0f888f4c50ed>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  return tuple(discrete_state.astype(np.int))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0\n",
            "Time Average: 9.33837890625e-06\n",
            "Mean Reward: 0.026\n",
            "Time Average: 0.0010216608047485351\n",
            "Mean Reward: 22.487\n",
            "Episode: 2000\n",
            "Time Average: 0.0009970450401306152\n",
            "Mean Reward: 22.187\n",
            "Time Average: 0.0010070602893829347\n",
            "Mean Reward: 22.137\n",
            "Episode: 4000\n",
            "Time Average: 0.0010176208019256592\n",
            "Mean Reward: 22.242\n",
            "Time Average: 0.001014951229095459\n",
            "Mean Reward: 22.424\n",
            "Episode: 6000\n",
            "Time Average: 0.0010086288452148438\n",
            "Mean Reward: 22.224\n",
            "Time Average: 0.001008234977722168\n",
            "Mean Reward: 21.704\n",
            "Episode: 8000\n",
            "Time Average: 0.0010095551013946534\n",
            "Mean Reward: 22.554\n",
            "Time Average: 0.0010208215713500978\n",
            "Mean Reward: 22.675\n",
            "Episode: 10000\n",
            "Time Average: 0.0015083343982696532\n",
            "Mean Reward: 22.573\n",
            "Epsilon: 0.9753093024395111\n",
            "Epsilon: 0.9512282354250458\n",
            "Time Average: 0.0013023552894592286\n",
            "Mean Reward: 23.259\n",
            "Episode: 12000\n",
            "Epsilon: 0.9048351558698463\n",
            "Time Average: 0.0011367733478546142\n",
            "Mean Reward: 24.516\n",
            "Epsilon: 0.8607047486686201\n",
            "Time Average: 0.0011609058380126953\n",
            "Mean Reward: 25.384\n",
            "Episode: 14000\n",
            "Epsilon: 0.818726659298009\n",
            "Time Average: 0.0012663259506225585\n",
            "Mean Reward: 27.338\n",
            "Time Average: 0.0013824000358581543\n",
            "Mean Reward: 30.509\n",
            "Episode: 16000\n",
            "Epsilon: 0.7408126643807126\n",
            "Time Average: 0.001440093994140625\n",
            "Mean Reward: 31.161\n",
            "Epsilon: 0.7046819235193919\n",
            "Time Average: 0.001530564308166504\n",
            "Mean Reward: 33.567\n",
            "Episode: 18000\n",
            "Epsilon: 0.6703133426452782\n",
            "Time Average: 0.0020020325183868408\n",
            "Mean Reward: 37.404\n",
            "Epsilon: 0.6376209781063321\n",
            "Time Average: 0.0022813968658447264\n",
            "Mean Reward: 39.08\n",
            "Epsilon: 0.6218776713776856\n",
            "Episode: 20000\n",
            "Time Average: 0.0020383949279785154\n",
            "Mean Reward: 42.765\n",
            "Time Average: 0.0021648683547973633\n",
            "Mean Reward: 46.163\n",
            "Epsilon: 0.5626967797130051\n",
            "Episode: 22000\n",
            "Epsilon: 0.5488034037068503\n",
            "Time Average: 0.0024231245517730713\n",
            "Mean Reward: 50.971\n",
            "Epsilon: 0.5352530648457575\n",
            "Time Average: 0.002762321710586548\n",
            "Mean Reward: 53.649\n",
            "Episode: 24000\n",
            "Epsilon: 0.4965766133349901\n",
            "Time Average: 0.003188316822052002\n",
            "Mean Reward: 59.389\n",
            "Epsilon: 0.484315790359524\n",
            "Epsilon: 0.47235769565598784\n",
            "Time Average: 0.0029240336418151856\n",
            "Mean Reward: 64.42\n",
            "Epsilon: 0.4606948546521764\n",
            "Episode: 26000\n",
            "Epsilon: 0.44931997732828616\n",
            "Time Average: 0.0032137629985809326\n",
            "Mean Reward: 69.76\n",
            "Epsilon: 0.43822595366018774\n",
            "Time Average: 0.00410642957687378\n",
            "Mean Reward: 74.968\n",
            "Episode: 28000\n",
            "Time Average: 0.0037696347236633302\n",
            "Mean Reward: 79.261\n",
            "Time Average: 0.003949250221252441\n",
            "Mean Reward: 85.893\n",
            "Epsilon: 0.3771831593051582\n",
            "Episode: 30000\n",
            "Epsilon: 0.3678702439938449\n",
            "Time Average: 0.004737268209457398\n",
            "Mean Reward: 88.081\n",
            "Epsilon: 0.3587872710578896\n",
            "Time Average: 0.004157498359680176\n",
            "Mean Reward: 89.444\n",
            "Epsilon: 0.3412885827413639\n",
            "Episode: 32000\n",
            "Time Average: 0.005199816465377808\n",
            "Mean Reward: 109.112\n",
            "Epsilon: 0.32464333633178233\n",
            "Epsilon: 0.31662766589938623\n",
            "Time Average: 0.005430480480194092\n",
            "Mean Reward: 108.044\n",
            "Epsilon: 0.30880990796138097\n",
            "Episode: 34000\n",
            "Epsilon: 0.3011851759202241\n",
            "Time Average: 0.0047567589282989505\n",
            "Mean Reward: 103.388\n",
            "Epsilon: 0.28649584342677675\n",
            "Time Average: 0.006185091972351074\n",
            "Mean Reward: 122.312\n",
            "Episode: 36000\n",
            "Epsilon: 0.27252293559946306\n",
            "Time Average: 0.005457701444625854\n",
            "Mean Reward: 122.075\n",
            "Epsilon: 0.2657941542182801\n",
            "Epsilon: 0.25923151114313064\n",
            "Time Average: 0.006316936016082764\n",
            "Mean Reward: 126.442\n",
            "Episode: 38000\n",
            "Epsilon: 0.24658833291124824\n",
            "Time Average: 0.006206043243408203\n",
            "Mean Reward: 137.988\n",
            "Epsilon: 0.24049989496139146\n",
            "Time Average: 0.006705292224884033\n",
            "Mean Reward: 133.15\n",
            "Epsilon: 0.22877029070403326\n",
            "Episode: 40000\n",
            "Time Average: 0.008060322761535645\n",
            "Mean Reward: 160.42\n",
            "Time Average: 0.007056458234786987\n",
            "Mean Reward: 156.143\n",
            "Epsilon: 0.206999401647574\n",
            "Episode: 42000\n",
            "Epsilon: 0.20188844202629158\n",
            "Time Average: 0.007809514760971069\n",
            "Mean Reward: 158.943\n",
            "Epsilon: 0.19690367556326213\n",
            "Epsilon: 0.192041986461381\n",
            "Time Average: 0.007769211530685425\n",
            "Mean Reward: 159.649\n",
            "Epsilon: 0.18730033585474753\n",
            "Episode: 44000\n",
            "Time Average: 0.007079911947250366\n",
            "Mean Reward: 156.687\n",
            "Epsilon: 0.17816536796962992\n",
            "Epsilon: 0.17376634075333858\n",
            "Time Average: 0.008157148361206054\n",
            "Mean Reward: 163.466\n",
            "Epsilon: 0.16947592858760505\n",
            "Episode: 46000\n",
            "Time Average: 0.008309793949127198\n",
            "Mean Reward: 168.626\n",
            "Epsilon: 0.15722989402047993\n",
            "Time Average: 0.006972343683242798\n",
            "Mean Reward: 153.434\n",
            "Epsilon: 0.15334777825975254\n",
            "Episode: 48000\n",
            "Epsilon: 0.1495615146451681\n",
            "Time Average: 0.008899158000946044\n",
            "Mean Reward: 179.782\n",
            "Epsilon: 0.1422671356634204\n",
            "Time Average: 0.010137156248092652\n",
            "Mean Reward: 205.332\n",
            "Episode: 50000\n",
            "Epsilon: 0.13532851641609098\n",
            "Time Average: 0.00910326361656189\n",
            "Mean Reward: 187.147\n",
            "Epsilon: 0.12872830587316755\n",
            "Time Average: 0.00862857699394226\n",
            "Mean Reward: 188.553\n",
            "Epsilon: 0.12554991420537906\n",
            "Episode: 52000\n",
            "Time Average: 0.010787102222442627\n",
            "Mean Reward: 220.823\n",
            "Epsilon: 0.11647789670960881\n",
            "Time Average: 0.009837552070617676\n",
            "Mean Reward: 200.314\n",
            "Epsilon: 0.11360197618947\n",
            "Episode: 54000\n",
            "Epsilon: 0.11079706415310193\n",
            "Time Average: 0.009703406810760498\n",
            "Mean Reward: 199.95\n",
            "Time Average: 0.009270293235778809\n",
            "Mean Reward: 190.494\n",
            "Episode: 56000\n",
            "Epsilon: 0.10025307881289336\n",
            "Time Average: 0.01059231162071228\n",
            "Mean Reward: 222.404\n",
            "Epsilon: 0.09536355925511658\n",
            "Time Average: 0.011051034450531006\n",
            "Mean Reward: 238.434\n",
            "Episode: 58000\n",
            "Time Average: 0.010523850917816163\n",
            "Mean Reward: 226.187\n",
            "Epsilon: 0.08628830100290674\n",
            "Time Average: 0.01238310146331787\n",
            "Mean Reward: 255.087\n",
            "Episode: 60000\n",
            "Epsilon: 0.08207986830082019\n",
            "Time Average: 0.010945157527923584\n",
            "Mean Reward: 229.74\n"
          ]
        }
      ]
    }
  ]
}